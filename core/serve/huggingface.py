from transformers import pipeline
import torch
import threading

# --- Optimized module-level global variables ---

# 1. Move model path configuration outside as module configuration for clarity
_MODEL_MAPPING = {
    "gpt-oss-20b": "/mnt/data/gpt-oss-20b",
    "gpt-oss-120b": "/mnt/data/gpt-oss-120b",
    "Qwen2___5-3B-Instruct": "/mnt/data/Qwen2___5-3B-Instruct",
}

# 2. Global cache for storing loaded pipeline objects
_model_cache = {}

# 3. Thread lock to ensure no conflicts occur when loading models in multi-threaded environments (e.g. web servers)
_cache_lock = threading.Lock()


def _get_model_pipeline(model_name: str):
    """
    An internal helper function responsible for loading and caching model pipelines.
    Implements thread-safe lazy loading.
    """
    # Fast check (without lock): If model is already in cache, return directly
    if model_name in _model_cache:
        print(f"Using cached model pipeline for '{model_name}'.")
        return _model_cache[model_name]

    # If model is not in cache, acquire lock to load model
    with _cache_lock:
        # Double-checked locking: After acquiring lock, check again if model has been loaded by another thread
        if model_name in _model_cache:
            print(f"Using cached model pipeline for '{model_name}' (retrieved after lock).")
            return _model_cache[model_name]

        # --- Model loading logic ---
        print(f"Loading model '{model_name}' for the first time...")
        if model_name not in _MODEL_MAPPING:
            raise ValueError(f"Unsupported model: {model_name}. Please add it to _MODEL_MAPPING.")
        
        model_path = _MODEL_MAPPING[model_name]
        
        try:
            pipe = pipeline(
                "text-generation",
                model=model_path,
                torch_dtype="auto",
                device_map="auto",
            )
            # Store the loaded pipeline in cache
            _model_cache[model_name] = pipe
            print(f"Model '{model_name}' loaded and cached successfully.")
            return pipe
        except Exception as e:
            print(f"Error loading model {model_name} from {model_path}: {e}")
            # Raise exception to inform caller of loading failure
            raise

def call_api(model: str, messages: list, temperature: float = 0.1, max_tokens: int = 256, **kwargs):
    """
    Call local transformers model for inference.
    This function will efficiently reuse models already loaded into memory.

    :param model: Model name (e.g., "Qwen2___5-3B-Instruct")
    :param messages: List of messages
    :param temperature: Generation temperature
    :param max_new_tokens: Maximum number of tokens to generate
    :param kwargs: Other parameters passed to pipeline
    :return: Text generated by the model
    """
    # 1. Get model pipeline (this step handles loading and caching)
    pipe = _get_model_pipeline(model)

    # 2. Call model to generate text
    do_sample = True if temperature > 0 else False
    
    outputs = pipe(
        messages,
        max_new_tokens=max_tokens,
        temperature=temperature,
        do_sample=do_sample,
        **kwargs
    )
    
    # 3. Extract and return generated text
    #    Hugging Face pipeline for conversational models returns the full conversation history.
    #    The last message is the assistant's reply.
    generated_message_list = outputs[0]["generated_text"]
    if isinstance(generated_message_list, list) and len(generated_message_list) > 0:
        return generated_message_list[-1]["content"]
    else:
        # Provide fallback logic in case output format does not meet expectations
        print(f"Warning: Unexpected output format from pipeline: {generated_message_list}")
        return str(generated_message_list)


# --- Example: How to properly test cache effectiveness ---
# Cache effect can only be seen when calling call_api continuously in the same script.
if __name__ == "__main__":
    model_name = "Qwen2___5-3B-Instruct"
    
    # First call
    print("--- First call ---")
    messages_1 = [{"role": "user", "content": "Introduce the development of artificial intelligence briefly."}]
    response_1 = call_api(model_name, messages_1, max_tokens=128)
    print("\n[Response 1]:")
    print(response_1)
    
    print("\n" + "="*80 + "\n")
    
    # Second call (same model)
    print("--- Second call (should be much faster and use cache) ---")
    messages_2 = [{"role": "user", "content": "What's the capital of France?"}]
    response_2 = call_api(model_name, messages_2, max_tokens=32)
    print("\n[Response 2]:")
    print(response_2)